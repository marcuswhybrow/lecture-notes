.. _G53MLE09:

============================
09 - Support Vector Machines
============================

Perceptron Revisited: Linear Separators
=======================================

In perceptrons we linearly separate the feature space. Points of one side of the line are classified differently to points on the other side.

However there are many lines which could separate the points correctly. So which of the linear separators is optimal?

Classification Margin
=====================

Te distance from an example point :math:`x_i` and the linear separator is:

.. math:: r = \frac{w^T x_i + b}{|w|}

where :math:`w` is the vector which represents the separator and :math:`b` is the bias.

The examples which are closest to the linear separator are called **support vectors**.

The **margin** :math:`p` of the linear separator is the distance between the linear separator and its support vectors.

Maximum Margin Classification
=============================

Once we have the linear separator, the support vectors and the margin, we need to maximise the margin according to intuition and PAC (Probably Approximately Correct) theory.

This means that the line will divide the feature space as centrally as possible.

The Optimisation Problem Solution
=================================

By solving something called the optimisation problem we will have identified the support vectors.

Soft Margin Classification
==========================

What happens if the training set is not linearly separable?

*Slack variables* can be added to allow misclassification of difficult or noisy examples, resulting in a so called **soft margin**.
