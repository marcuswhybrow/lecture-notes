.. _G53MLE03:

==========================
03 - ADLINE and Delta Rule
==========================

Adaptive Linear Element (ADLINE) vs Perceptron.

ADLINE just outputs the weighted sum directly as apposed to the Perceptron with passes that weighted sum through a threshold as well.

The ADLINE and Delta Rule
=========================

When the problem is not linearly separable the perceptron will fail to converge. ADLINE can overcome this difficulty by finding a best fit approximation to the target.

The ADLINE Error Function
=========================

We have training paris :math:`<X(k), d(k), k=1,2,...,K>` where K is the number of training samples, the training error specifies the difference between the output of the ADLINE and the desired target.

The error function :math:`E` is defined as:

.. math:: E (W) = \frac{1}{2} \sum_{k=1}^K (d_k - y_k^2)

where:

.. math:: y_k = W^T X_k

The difference between the actual output and the designed output is used by the error function in order to determine how close we got to approximating the desired function/weights.

The smaller :math:`E(W)` is the closer the approximation is to the perfect function.

So the training task is to find :math:`W` such that :math:`E(W)` is minimised.

The differing values of :math:`W` creates a so called *error surface* which is visualisable if there are 2 or 3 weights. In 2D this surface would appear as valleys and mountains etc. spanning the hypothesis space. Our aim is to find the point which is the lowest in that error surface.

The Gradient Descent Rule
=========================

If you imagine a 2D graph displaying the error surface (in this case a line) we need to move :math:`w(0)` to a new value, such that :math:`E(w(new)) < E(w(0))`.

This is easy if you are looking at the line, you can visually pick out the lowest point. To do this programmatically and without a priori knowledge we want to travel down negative gradients.

This equates to moving in the opposite direction to the gradient, i.e. if the gradient going forward is positive (going up) we must move in the negative direction (backwards), which will take us downwards.

Mathematically we can say:

.. math:: w_{new} = w_{old} - \Delta sign 

.. math:: \Delta E(W) = [\frac{\delta E}{ \delta w_1}, \frac{\delta E}{ \delta w_2}, ... , \frac{\delta E}{ \delta w_n}]

The gradient training rule is for each individual weight is:

.. math:: w_i = w_i - n \frac{\delta E}{\delta w_i}

where :math:`n` is the training rate.

The gradient descent training procedure is:

#. Initialise :math:`w_i` to small values (for example in the range of -1 to 1) and choose a learning rate (e.g. n = 0.1).
#. Until the termination condition is met, do
    * For all training sample pairs :math:`<X_k, d_k>` input the instance :math:`X_k` and compute:
    
    .. math:: \delta_i = i \sum_{k=1}^K (d_k - y_k) x_i(k)
    
    * For each weight :math:`w_i`, do
    
        .. math:: w_i = w_i + n \delta_i

Stochastic (Incremental) Gradient Descent
=========================================

*...*

Termination of Training
=======================

*...*

Gradient Descent Training
=========================

