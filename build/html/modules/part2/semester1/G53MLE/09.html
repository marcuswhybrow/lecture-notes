

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>09 - Support Vector Machines &mdash; Lecture Notes v1.0 documentation</title>
    <link rel="stylesheet" href="../../../../static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../../../static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../static/jquery.js"></script>
    <script type="text/javascript" src="../../../../static/underscore.js"></script>
    <script type="text/javascript" src="../../../../static/doctools.js"></script>
    <link rel="top" title="Lecture Notes v1.0 documentation" href="../../../../index.html" />
    <link rel="up" title="G53MLE Machine Learning" href="index.html" />
    <link rel="next" title="G53SRP Systems and Real-Time Programming" href="../G53SRP/index.html" />
    <link rel="prev" title="08 - Data Processing and Representation Principal Component Analysis (PCA)" href="08.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../G53SRP/index.html" title="G53SRP Systems and Real-Time Programming"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="08.html" title="08 - Data Processing and Representation Principal Component Analysis (PCA)"
             accesskey="P">previous</a> |</li>
        <li><a href="../../../../index.html">Lecture Notes v1.0 documentation</a> &raquo;</li>
          <li><a href="index.html" accesskey="U">G53MLE Machine Learning</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="support-vector-machines">
<span id="g53mle09"></span><h1>09 - Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">¶</a></h1>
<div class="section" id="perceptron-revisited-linear-separators">
<h2>Perceptron Revisited: Linear Separators<a class="headerlink" href="#perceptron-revisited-linear-separators" title="Permalink to this headline">¶</a></h2>
<p>In perceptrons we linearly separate the feature space. Points of one side of the line are classified differently to points on the other side.</p>
<p>However there are many lines which could separate the points correctly. So which of the linear separators is optimal?</p>
</div>
<div class="section" id="classification-margin">
<h2>Classification Margin<a class="headerlink" href="#classification-margin" title="Permalink to this headline">¶</a></h2>
<p>Te distance from an example point <img class="math" src="../../../../images/math/67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png" alt="x_i"/> and the linear separator is:</p>
<div class="math">
<p><img src="../../../../images/math/2788312dc4b86da399329a56a270c394db3066cc.png" alt="r = \frac{w^T x_i + b}{|w|}" /></p>
</div><p>where <img class="math" src="../../../../images/math/9ee4b825a2e36ae093ed7be5e4851ef453b34914.png" alt="w"/> is the vector which represents the separator and <img class="math" src="../../../../images/math/8136a7ef6a03334a7246df9097e5bcc31ba33fd2.png" alt="b"/> is the bias.</p>
<p>The examples which are closest to the linear separator are called <strong>support vectors</strong>.</p>
<p>The <strong>margin</strong> <img class="math" src="../../../../images/math/36f73fc1312ee0349b3f3a0f3bd9eb5504339011.png" alt="p"/> of the linear separator is the distance between the linear separator and its support vectors.</p>
</div>
<div class="section" id="maximum-margin-classification">
<h2>Maximum Margin Classification<a class="headerlink" href="#maximum-margin-classification" title="Permalink to this headline">¶</a></h2>
<p>Once we have the linear separator, the support vectors and the margin, we need to maximise the margin according to intuition and PAC (Probably Approximately Correct) theory.</p>
<p>This means that the line will divide the feature space as centrally as possible.</p>
</div>
<div class="section" id="the-optimisation-problem-solution">
<h2>The Optimisation Problem Solution<a class="headerlink" href="#the-optimisation-problem-solution" title="Permalink to this headline">¶</a></h2>
<p>By solving something called the optimisation problem we will have identified the support vectors.</p>
</div>
<div class="section" id="soft-margin-classification">
<h2>Soft Margin Classification<a class="headerlink" href="#soft-margin-classification" title="Permalink to this headline">¶</a></h2>
<p>What happens if the training set is not linearly separable?</p>
<p><em>Slack variables</em> can be added to allow misclassification of difficult or noisy examples, resulting in a so called <strong>soft margin</strong>.</p>
</div>
<div class="section" id="linear-svms-overview">
<h2>Linear SVMs: Overview<a class="headerlink" href="#linear-svms-overview" title="Permalink to this headline">¶</a></h2>
<p>This classifier (SVM, Support Vector Machines) is a <em>separating hyperplane</em>. The most &#8220;important&#8221; training points are support vectors; they define the hyperplane.</p>
<p>Quadratic optimisation algorithms can identify which training points <img class="math" src="../../../../images/math/67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png" alt="x_i"/> are support vectors with non-zero Lagrangian multipliers <img class="math" src="../../../../images/math/412787c048e28774dc63fc27db42dc52ca858de7.png" alt="a_i"/>.</p>
<p>Both in the dual formulation of the problem and in the solution training points appear only inside inner products.</p>
</div>
<div class="section" id="non-linear-svms">
<h2>Non-Linear SVMs<a class="headerlink" href="#non-linear-svms" title="Permalink to this headline">¶</a></h2>
<p>Datasets that are linearly separable with some noise work out great:</p>
<img alt="../../../../images/linearly-separable-svm.png" src="../../../../images/linearly-separable-svm.png" />
<p>If the data is not linearly separable like this:</p>
<img alt="../../../../images/not-linearly-separable-svm.png" src="../../../../images/not-linearly-separable-svm.png" />
<p>We can map the data to a higher-dimensional space:</p>
<img alt="../../../../images/mapped-to-higher-dimension.png" src="../../../../images/mapped-to-higher-dimension.png" />
<p>In this form we <strong>can</strong> linearly separate the different classes.</p>
</div>
<div class="section" id="non-linear-svms-feature-spaces">
<h2>Non-Linear SVMs: Feature Spaces<a class="headerlink" href="#non-linear-svms-feature-spaces" title="Permalink to this headline">¶</a></h2>
<p>The general idea is that the original feature space can always be mapped to some higher dimensional feature space where the training set is separable</p>
</div>
<div class="section" id="the-kernel-trick">
<h2>The &#8220;Kernel Trick&#8221;<a class="headerlink" href="#the-kernel-trick" title="Permalink to this headline">¶</a></h2>
<p>The linear classifier relies on the inner product between vectors <img class="math" src="../../../../images/math/6acb2d6fbeaa8b4138a60c4e51cfd5d51a3cd9bc.png" alt="K(x_i, x_j) = x_i^T x_j"/>.</p>
<p>If every datapoint is mapped into high-dimensional space via some transformation <img class="math" src="../../../../images/math/2c175f60eecef1de7560c3bdea495d69f26f719d.png" alt="\phi"/> such that <img class="math" src="../../../../images/math/b4d11daa57c1328a205939def10291754edbb3ce.png" alt="x \rightarrow \phi (x)"/>, the inner product becomes:</p>
<div class="math">
<p><img src="../../../../images/math/94349697194e5b7d91364297a4226ac2e2b05d2c.png" alt="K(x_i, x_j) = \phi (x_i)^T \phi(x_j)" /></p>
</div><p>A <em>kernel function</em> is a function that is equivalent to an inner product in some feature space. A kernel function <em>implicitly</em> maps data to a high-dimensional space (without the need to compute each <img class="math" src="../../../../images/math/42c0bd264b62986d6b2a935c5775170a4ed6e9fd.png" alt="\phi (x)"/> explicitly).</p>
</div>
<div class="section" id="what-functions-are-kernels">
<h2>What Functions are Kernels?<a class="headerlink" href="#what-functions-are-kernels" title="Permalink to this headline">¶</a></h2>
<p>For some functions <img class="math" src="../../../../images/math/a159a2aa1f9fedff90eae2cac7190ea20327570b.png" alt="K(x_i, x_j)"/> checking that <img class="math" src="../../../../images/math/cc6b0ffecbde4830fde1790301d2e44904d701ff.png" alt="K(x_i, x_j) = \phi (x_i)^T \phi(x_j)"/> can be cumbersome.</p>
<p>Mercer&#8217;s theorem says:</p>
<blockquote>
&#8220;Every semi-positive definite symmetric function is a kernel&#8221;</blockquote>
</div>
<div class="section" id="unique-feature-of-svm-s-and-kernel-methods">
<h2>Unique Feature of SVM&#8217;s and Kernel Methods<a class="headerlink" href="#unique-feature-of-svm-s-and-kernel-methods" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Are explicitly based on a theoretical model of learning</li>
<li>Come with theoretical guarantees about their performance</li>
<li>Have a modular design that allows one to separately implement and design their components</li>
<li>Are not affected by local minima</li>
<li>Do not suffer from the curse of dimensionality</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">09 - Support Vector Machines</a><ul>
<li><a class="reference internal" href="#perceptron-revisited-linear-separators">Perceptron Revisited: Linear Separators</a></li>
<li><a class="reference internal" href="#classification-margin">Classification Margin</a></li>
<li><a class="reference internal" href="#maximum-margin-classification">Maximum Margin Classification</a></li>
<li><a class="reference internal" href="#the-optimisation-problem-solution">The Optimisation Problem Solution</a></li>
<li><a class="reference internal" href="#soft-margin-classification">Soft Margin Classification</a></li>
<li><a class="reference internal" href="#linear-svms-overview">Linear SVMs: Overview</a></li>
<li><a class="reference internal" href="#non-linear-svms">Non-Linear SVMs</a></li>
<li><a class="reference internal" href="#non-linear-svms-feature-spaces">Non-Linear SVMs: Feature Spaces</a></li>
<li><a class="reference internal" href="#the-kernel-trick">The &#8220;Kernel Trick&#8221;</a></li>
<li><a class="reference internal" href="#what-functions-are-kernels">What Functions are Kernels?</a></li>
<li><a class="reference internal" href="#unique-feature-of-svm-s-and-kernel-methods">Unique Feature of SVM&#8217;s and Kernel Methods</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="08.html"
                        title="previous chapter">08 - Data Processing and Representation Principal Component Analysis (PCA)</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../G53SRP/index.html"
                        title="next chapter">G53SRP Systems and Real-Time Programming</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../../../sources/modules/part2/semester1/G53MLE/09.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../G53SRP/index.html" title="G53SRP Systems and Real-Time Programming"
             >next</a> |</li>
        <li class="right" >
          <a href="08.html" title="08 - Data Processing and Representation Principal Component Analysis (PCA)"
             >previous</a> |</li>
        <li><a href="../../../../index.html">Lecture Notes v1.0 documentation</a> &raquo;</li>
          <li><a href="index.html" >G53MLE Machine Learning</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010, Marcus Whybrow.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.4.
    </div>
  </body>
</html>