

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>05 - Bayesian Learning &mdash; Lecture Notes v1.0 documentation</title>
    <link rel="stylesheet" href="../../../../static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../../../static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../static/jquery.js"></script>
    <script type="text/javascript" src="../../../../static/underscore.js"></script>
    <script type="text/javascript" src="../../../../static/doctools.js"></script>
    <link rel="top" title="Lecture Notes v1.0 documentation" href="../../../../index.html" />
    <link rel="up" title="G53MLE Machine Learning" href="index.html" />
    <link rel="next" title="06 - K-Nearest Neighbour Classifier" href="06.html" />
    <link rel="prev" title="04 - Multilayer Perceptrons" href="04.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="06.html" title="06 - K-Nearest Neighbour Classifier"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="04.html" title="04 - Multilayer Perceptrons"
             accesskey="P">previous</a> |</li>
        <li><a href="../../../../index.html">Lecture Notes v1.0 documentation</a> &raquo;</li>
          <li><a href="index.html" accesskey="U">G53MLE Machine Learning</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="bayesian-learning">
<span id="g53mle05"></span><h1>05 - Bayesian Learning<a class="headerlink" href="#bayesian-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="discrete-random-variables">
<h2>Discrete Random Variables<a class="headerlink" href="#discrete-random-variables" title="Permalink to this headline">¶</a></h2>
<p>If <img class="math" src="../../../../images/math/019e9892786e493964e145e7c5cf7b700314e53b.png" alt="A"/> is a boolean valued random variable denoting an event, and there is some degree of uncertainty as to whether <img class="math" src="../../../../images/math/019e9892786e493964e145e7c5cf7b700314e53b.png" alt="A"/> occurs.</p>
<p>For example <img class="math" src="../../../../images/math/019e9892786e493964e145e7c5cf7b700314e53b.png" alt="A"/> could be:</p>
<ul class="simple">
<li>The US president in 2023 will be Male</li>
<li>You wake up tomorrow with a headache</li>
<li>You have Ebola</li>
</ul>
</div>
<div class="section" id="probabilities">
<h2>Probabilities<a class="headerlink" href="#probabilities" title="Permalink to this headline">¶</a></h2>
<p>We write <img class="math" src="../../../../images/math/6f70328223696bf019cbd1df694fcae4c01a995d.png" alt="P(A)"/> as &#8220;the fraction of possible worlds in which <img class="math" src="../../../../images/math/019e9892786e493964e145e7c5cf7b700314e53b.png" alt="A"/> is true&#8221;.</p>
</div>
<div class="section" id="axioms-of-probability-theory">
<h2>Axioms of Probability Theory<a class="headerlink" href="#axioms-of-probability-theory" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p class="first">All probabilities lie between 0 and 1</p>
<blockquote>
<div class="math">
<p><img src="../../../../images/math/2c68d5c90469a7e862939790c880c932c988b5b5.png" alt="0 &lt;= P(A) &lt;= 1" /></p>
</div></blockquote>
</li>
<li><p class="first">A true proposition has probability 1, and a false proposition has probability 0</p>
<blockquote>
<div class="math">
<p><img src="../../../../images/math/3eac0e27ea059f69b78be2191d9c4348e3a73c0d.png" alt="P(true) = 1

P(false) = 0" /></p>
</div></blockquote>
</li>
</ol>
<p># THe probability of disjunction is:</p>
<blockquote>
<div class="math">
<p><img src="../../../../images/math/1cc900a073e90677eddc46e1d31f05f0697f8688.png" alt="P(A \vee B) = P(A) + P(B) - P(A \wedge B)" /></p>
</div></blockquote>
</div>
<div class="section" id="theorems-from-the-axioms">
<h2>Theorems from the Axioms<a class="headerlink" href="#theorems-from-the-axioms" title="Permalink to this headline">¶</a></h2>
<div class="math">
<p><img src="../../../../images/math/610f83447547bbdebe44aa68b8a5a1509ad1a4c5.png" alt="P( \lnot A) = P( \sim A) = 1 - P(A)

P(A) = P(A \wedge B) + P(A \wedge \sim B)" /></p>
</div></div>
<div class="section" id="multivalued-random-variables">
<h2>Multivalued Random Variables<a class="headerlink" href="#multivalued-random-variables" title="Permalink to this headline">¶</a></h2>
<p>Suppose <img class="math" src="../../../../images/math/019e9892786e493964e145e7c5cf7b700314e53b.png" alt="A"/> can take on exactly one value of the set <img class="math" src="../../../../images/math/333964964aa2d0609278eff4198f0b5c47bf7bff.png" alt="{v_1, v_2, ..., v_k}"/>:</p>
<div class="math">
<p><img src="../../../../images/math/6ef4f7353b3c0e1528741274cc6c19b9bed44c9e.png" alt="P(A = v_i \wedge A = v_j) = 0 if i \neq j

P(A = v_1 \vee A = V-2 \vee ... \vee A = v_k) = 1 = \sum_{j=1}^i P(A = v_j)" /></p>
</div></div>
<div class="section" id="conditional-probability">
<h2>Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h2>
<p>The probability <img class="math" src="../../../../images/math/a933631c847ef31f4523f0160ebc84e9612ee6c1.png" alt="P(A|B)"/> is the fraction of worlds in which <img class="math" src="../../../../images/math/ff5fb3d775862e2123b007eb4373ff6cc1a34d4e.png" alt="B"/> is true that also have A as true.</p>
<p>For example:
* <img class="math" src="../../../../images/math/b1902d279ba37d60bdce4e0e987b7cd19d48974e.png" alt="H"/> is &#8220;Having a headache&#8221;
* <img class="math" src="../../../../images/math/a055f405829e64a3b70253ab67cb45ed6ed5bb29.png" alt="F"/> is &#8220;Coming down with Flu&#8221;</p>
<p>If:</p>
<div class="math">
<p><img src="../../../../images/math/bc82b6270b2c90c4869da7395fb968506e3245f5.png" alt="P(H) = \frac{1}{10}
P(F) = \frac{1}{40}
P(H|F) = \frac{1}{2}" /></p>
</div><p>Remember that <img class="math" src="../../../../images/math/420cc117ac14be836ba6cadebf4155acc8eba715.png" alt="P(H|F)"/> means &#8220;what is the probability of <img class="math" src="../../../../images/math/b1902d279ba37d60bdce4e0e987b7cd19d48974e.png" alt="H"/> if <img class="math" src="../../../../images/math/a055f405829e64a3b70253ab67cb45ed6ed5bb29.png" alt="F"/> is true&#8221;.</p>
</div>
<div class="section" id="definition-of-conditional-probability">
<h2>Definition of Conditional Probability<a class="headerlink" href="#definition-of-conditional-probability" title="Permalink to this headline">¶</a></h2>
<div class="math">
<p><img src="../../../../images/math/56b3a4c53b33ab99e37d03ee923c24105d68369a.png" alt="P(H|F) = \frac{P(H \wedge F)}{P(F)}" /></p>
</div><p>And also <em>The Chain Rule</em></p>
<div class="math">
<p><img src="../../../../images/math/c37529da0a4ea239eda90d55c750a2e334b5cf6c.png" alt="P(A \wedge B) = P(A|B) P(B)" /></p>
</div></div>
<div class="section" id="probabilistic-inference">
<h2>Probabilistic Inference<a class="headerlink" href="#probabilistic-inference" title="Permalink to this headline">¶</a></h2>
<p>It is <strong>not</strong> good reasoning to say that &#8220;I have woken up with a headache, thus there is a 50% chance I have of coming down with the flu since 50% of flus lead to a headache&#8221;.</p>
<div class="math">
<p><img src="../../../../images/math/aa6bd4eaa9226bf4761dbc777b93434c7a0efaf5.png" alt="P(F \wedge H) = P(H|F) P(F) = \frac{1}{80}" /></p>
</div><p>However:</p>
<div class="math">
<p><img src="../../../../images/math/b7b8fa0fbdcd0cedfa6775f452fbc976d5f6217f.png" alt="P(F|H) = \frac{P(F \wedge H)}{P(H)} = \frac{\frac{1}{80}}{\frac{1}{10}} = \frac{1}{8}" /></p>
</div></div>
<div class="section" id="bayes-rule">
<h2>Bayes Rule<a class="headerlink" href="#bayes-rule" title="Permalink to this headline">¶</a></h2>
<p><strong>Bayes, Thomas (1763)</strong> AN essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London.</p>
<p>So Bayes Rule is:</p>
<div class="math">
<p><img src="../../../../images/math/4b3f357456851a7f645fa346a5d4ab1d06b16834.png" alt="P(A|B) = \frac{P(A \wedge B)}{P(B)} = \frac{P(V|A)P(A)}{P(B)}" /></p>
</div></div>
<div class="section" id="id1">
<h2>Bayesian Learning<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>So we can use this rule in a machine learning context by first renaming the variable more appropriately:</p>
<div class="math">
<p><img src="../../../../images/math/40cb3d38c87ee1b05915063a64718d3546bd89d4.png" alt="P(h|x) = \frac{P(x|h)P(h)}{P(x)}" /></p>
</div><p>where <img class="math" src="../../../../images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/> is the data and <img class="math" src="../../../../images/math/8189a5b5a0917b8c93350827be4038af1839139d.png" alt="h"/> is the hypothesis.</p>
<ul class="simple">
<li><img class="math" src="../../../../images/math/3e68730c936e4d6959516f6226ac7cc77be734ab.png" alt="P(h)"/> is the probability of hypothesis <img class="math" src="../../../../images/math/8189a5b5a0917b8c93350827be4038af1839139d.png" alt="h"/> before seeing any data.</li>
<li><img class="math" src="../../../../images/math/8d686b89599e9d6d47b82b6ef4e20753d530b730.png" alt="P(x|h)"/> is the probability of the data if the hypothesis <img class="math" src="../../../../images/math/8189a5b5a0917b8c93350827be4038af1839139d.png" alt="h"/> is true.</li>
<li><img class="math" src="../../../../images/math/a281a95da216a5207079894e80b3094bdc40c20f.png" alt="P(h|x)"/> is the probability of hypothesis <img class="math" src="../../../../images/math/8189a5b5a0917b8c93350827be4038af1839139d.png" alt="h"/> after having see the data <img class="math" src="../../../../images/math/26eeb5258ca5099acf8fe96b2a1049c48c89a5e6.png" alt="x"/>.</li>
</ul>
<p><em>...</em></p>
</div>
<div class="section" id="an-illustrating-example">
<h2>An Illustrating Example<a class="headerlink" href="#an-illustrating-example" title="Permalink to this headline">¶</a></h2>
<p>A patient takes a lab test and the result comes back positive. It is known that the test returns a correct positive result in only 98% of cases and a correct negative result in only 97% of cases. Furthermore, only 0.008% of the entire population has this disease.</p>
<ol class="arabic simple">
<li>What is the probability that this patient has cancer?</li>
<li>What is the probability that he does not have cancer?</li>
<li>What is the diagnosis?</li>
</ol>
<p>The available data has two possible outcomes, either positive represented by a <img class="math" src="../../../../images/math/77cf26be132ef93923e082ee4153b2cb0ef44a50.png" alt="+"/> symbol and negative represented by a <img class="math" src="../../../../images/math/add22d9bda21e86390bc74fa4dde17730a442da7.png" alt="-"/> symbol.</p>
<p>The various probabilities are:</p>
<div class="math">
<p><img src="../../../../images/math/43762dbcf800c154cbbed068b4ab4a5026a15556.png" alt="P(cancer) = 0.008

P(\sim cancer) = 0.992

P(+ | cancer) = 0.98

P(- | cancer) = 0.02

P(+ | \sim cancer) = 0.03

P(- | \sim cancer) = 0.97" /></p>
</div><p>Now we have a new patient, whose test result is positive. Should we diagnose the patient as having cancer or not?</p>
</div>
<div class="section" id="choosing-hypotheses">
<h2>Choosing Hypotheses<a class="headerlink" href="#choosing-hypotheses" title="Permalink to this headline">¶</a></h2>
<p>Generally, we want the most probable hypothesis given the observed data. There are some terms people use in order to describe this process:</p>
<p>MAP Hypothesis - Maximum A Posteriori Hypothesis</p>
<p>ML Hypothesis - Maximum Likelihood Hypothesis</p>
</div>
<div class="section" id="maximum-a-posteriori-map">
<h2>Maximum a Posteriori (MAP)<a class="headerlink" href="#maximum-a-posteriori-map" title="Permalink to this headline">¶</a></h2>
<p>First remember we can use the Bayesian equation:</p>
<div class="math">
<p><img src="../../../../images/math/d6a21030058e712437bf9e6db03a523752c89876.png" alt="P(h|x) = \frac{P(x|h) P(h)}{P(x)}" /></p>
</div><p>So we know this much:</p>
<div class="math">
<p><img src="../../../../images/math/44cfafad7717b9e8e0ba940bd16672ae9ed205a9.png" alt="h_{MAP} = arg \max_{h \epsilon H} P(h|x)" /></p>
</div><p>Then we can replace the right hand side using the Bayesian equation:</p>
<div class="math">
<p><img src="../../../../images/math/b11acd7944d8e4e9f895cb687546707e02fcae5c.png" alt="h_{MAP} = arg \max_{h \epsilon H} \frac{P(x|h) P(h)}{P(x)}" /></p>
</div><p>Then we can drop the denominator since <img class="math" src="../../../../images/math/6efaaf74e576202fad965134bc49c6a34aac4c30.png" alt="P(x)"/> is independent of <img class="math" src="../../../../images/math/8189a5b5a0917b8c93350827be4038af1839139d.png" alt="h"/>, giving:</p>
<div class="math">
<p><img src="../../../../images/math/0bbffd133e3978c1546d47a86e2ac572652b2377.png" alt="h_{MAP} = arg \max_{h \epsilon H} P(x|h) P(h)" /></p>
</div></div>
<div class="section" id="maximum-likelihood-ml">
<h2>Maximum Likelihood (ML)<a class="headerlink" href="#maximum-likelihood-ml" title="Permalink to this headline">¶</a></h2>
<p>Assuming that each hypothesis in H (the hypothesis space) is equally probable (i.e. <img class="math" src="../../../../images/math/79f56e60eeeaf6d1566b24095135ff4907e6bf43.png" alt="P(h_i) = P(h_j)"/>), then for all <img class="math" src="../../../../images/math/34857b3ba74ce5cd8607f3ebd23e9015908ada71.png" alt="i"/> and <img class="math" src="../../../../images/math/8122aa89ea6e80784c6513d22787ad86e36ad0cc.png" alt="j"/>, we can drop <img class="math" src="../../../../images/math/3e68730c936e4d6959516f6226ac7cc77be734ab.png" alt="P(h)"/> in MAP. <img class="math" src="../../../../images/math/0b98725fe4c1d5155482b01ecf88402e3217e1a6.png" alt="P(d|h)"/> is often called the likelihood of data <img class="math" src="../../../../images/math/96ab646de7704969b91c76a214126b45f2b07b25.png" alt="d"/> given <img class="math" src="../../../../images/math/8189a5b5a0917b8c93350827be4038af1839139d.png" alt="h"/>. Any hypothesis that maximises <img class="math" src="../../../../images/math/0b98725fe4c1d5155482b01ecf88402e3217e1a6.png" alt="P(d|h)"/> is called the maximum likelihood hypothesis:</p>
<div class="math">
<p><img src="../../../../images/math/d0767e43d8f39f7d12f2e478a4afa54771ee8c67.png" alt="h_{ML} = arg \max_{h \epsilon H} P(x|h)" /></p>
</div></div>
<div class="section" id="does-the-patient-have-cancer-or-not">
<h2>Does the Patient Have Cancer or Not?<a class="headerlink" href="#does-the-patient-have-cancer-or-not" title="Permalink to this headline">¶</a></h2>
<p>So now, how do we diagnose the patient:</p>
<div class="math">
<p><img src="../../../../images/math/496fdca2153cafa076fc7d7b9dd82241bc6e3e9f.png" alt="P(+|cancer) P(cancer) = 0.98 * 0.008 = 0.078

P(+| \sim cancer) P (\sim cancer) = 0.03 * 0.992 = 0.298" /></p>
</div><p>So the probability of not having cancer is the higher, so the diagnosis is <strong>not cancer</strong>.</p>
</div>
<div class="section" id="bayesian-classifier">
<h2>Bayesian Classifier<a class="headerlink" href="#bayesian-classifier" title="Permalink to this headline">¶</a></h2>
<p>For the following, we will be using this table of data:</p>
<img alt="../../../../images/table.png" src="../../../../images/table.png" />
<p>The Bayesian approach to classifying new instances of <img class="math" src="../../../../images/math/6a47ca0fe7cb276abc022af6ac88ddae1a9d6894.png" alt="X"/> is to assign the class with the most probable target value <img class="math" src="../../../../images/math/ce58e4af225c93d08606c26554caaa5ae32edeba.png" alt="Y"/> (using a MAP classifier).</p>
<div class="math">
<p><img src="../../../../images/math/26c9c06f267da72ff153323ed88f905326a4c6bf.png" alt="Y = arg \max_{d_i \epsilon d} P(d_i | X)" /></p>
</div><p>This means use the maximum probability of each of the classes given we know the input vector <img class="math" src="../../../../images/math/6a47ca0fe7cb276abc022af6ac88ddae1a9d6894.png" alt="X"/>. So as before we use the Bayesian formula to substitute different values:</p>
<div class="math">
<p><img src="../../../../images/math/ea99ef80b18a3c15c3c6250ff917e6544cc282fa.png" alt="Y = arg \max_{d_i \epsilon d} \frac{P(X|d_i) P(d_i)}{P(X)}" /></p>
</div><p>And then we can drop the denominator as before:</p>
<div class="math">
<p><img src="../../../../images/math/b4d92baabee87efda787e86cc78283eb972fb11f.png" alt="Y = \max_{d_i \epsilon d} P(X|d_i) P(d_i)" /></p>
</div><p>Luckily <img class="math" src="../../../../images/math/284d02dde7ed455e84c7b12fdadd0a10065bbf3a.png" alt="P(d_i)"/> is easy to calculate by simply counting how many times each target value <img class="math" src="../../../../images/math/f1544d77330dc5a289ad6a42beeb236b8dd70a9a.png" alt="d_i"/> occurs in the training set.</p>
<p>Therefor we get the following probabilities for values of <img class="math" src="../../../../images/math/96ab646de7704969b91c76a214126b45f2b07b25.png" alt="d"/>:</p>
<div class="math">
<p><img src="../../../../images/math/33cd07107dd0130cc9e5e82cdcb8c511c5f1778e.png" alt="P(d = yes) = \frac{9}{14}

P(d = no) = \frac{5}{14}" /></p>
</div><p>The harder part is calculating <img class="math" src="../../../../images/math/4e74900bd1d9906a4212145bdb70b067bca9d7fb.png" alt="P(X|d_i)"/>. From the data we can see there are 5 columns (excluding the day column) the first and second with 3 values and the result with 2 values. Thus there are <img class="math" src="../../../../images/math/c7d5deb5ba5dbd8fc822d34ee593bb3ff29a94bb.png" alt="3 \times 3 \times 2 \times 2 \times 2 = 72"/> possible combinations.</p>
<p>To obtain a reliable estimate of the probability here, we would need to see each combination many times. However we don&#8217;t even see each combination 1 time as we have less than 72 observed instances.</p>
<p>Hence we need a very, very large training set, which in most cases is impossible to obtain.</p>
</div>
<div class="section" id="naive-bayes-classifier">
<h2>Naïve Bayes Classifier<a class="headerlink" href="#naive-bayes-classifier" title="Permalink to this headline">¶</a></h2>
<p>The Naïve Bayes classifier is based upon the simplifying assumptions that the attribute values are conditionally independent given the target value. The means we get the following:</p>
<div class="math">
<p><img src="../../../../images/math/6875abee82e055dddd5be3312d1af55b95ea8382.png" alt="P(X|d_i) = P(x_1, x_2, ..., x_n |d_i) = \prod_{k} P(x_k|d_i)" /></p>
</div><p>So the Naïve Bayes Classifier is:</p>
<div class="math">
<p><img src="../../../../images/math/5fb3487fe14dcfc5f949ab3abe1a46c89c5da05f.png" alt="Y = arg \max_{d_i \epsilon d} P(d_i) \prod_{k=1}^4 P(x_k|d_i)" /></p>
</div></div>
<div class="section" id="back-to-the-example">
<h2>Back to the Example<a class="headerlink" href="#back-to-the-example" title="Permalink to this headline">¶</a></h2>
<p>So we can now classify new instances for the table using the Naïve Bayes Classifier.</p>
</div>
<div class="section" id="estimating-probabilities">
<h2>Estimating Probabilities<a class="headerlink" href="#estimating-probabilities" title="Permalink to this headline">¶</a></h2>
<p>So far, we can estimate the probabilities using the fraction of times the event is observed to occur over the entire observed opportunities.</p>
<p>In the above example, we estimated:</p>
<div class="math">
<p><img src="../../../../images/math/4524e451913ada22a962dc07b8d14db69ae03029.png" alt="P(wind = strong | tennis = no) = \frac{N_c}{N}" /></p>
</div><p>where <img class="math" src="../../../../images/math/c996acd51c78d9ece0ff3d8e78057e9a77fbbb6e.png" alt="N = 5"/> is the total number of training samples for which <img class="math" src="../../../../images/math/1e843799d11b6d19b0f7bb7cc64cee6f9dd095d7.png" alt="tennis = no"/>, and <img class="math" src="../../../../images/math/1f2701dd482bb477191a4d5b9e20d84d7e06647c.png" alt="Nc"/> is the number of these for which <img class="math" src="../../../../images/math/273c827aead084a27934e94c9d22960a963bba71.png" alt="wind = strong"/>.</p>
<p>When <img class="math" src="../../../../images/math/38f90affde4558a00e3815a195c021eade94cb22.png" alt="N_c"/> is small, such approaches provide poor estimation. To avoid this difficulty, we can adopt the <strong>m-estimate</strong> of probability:</p>
<div class="math">
<p><img src="../../../../images/math/16db0e51785e7362200a0115446388003c4129ae.png" alt="\frac{N_c + mP}{N + m}" /></p>
</div><p>where <img class="math" src="../../../../images/math/4b4cade9ca8a2c8311fafcf040bc5b15ca507f52.png" alt="P"/> is the prior estimate of the probability we wish to estimate and <img class="math" src="../../../../images/math/f5047d1e0cbb50ec208923a22cd517c55100fa7b.png" alt="m"/> is a constant (which you choose) called the equivalent sample size.</p>
<p>A typical method for choosing <img class="math" src="../../../../images/math/4b4cade9ca8a2c8311fafcf040bc5b15ca507f52.png" alt="P"/> in the absence of other information is to assume uniform priors: If an attribute has <img class="math" src="../../../../images/math/8c325612684d41304b9751c175df7bcc0f61f64f.png" alt="k"/> possible values we set <img class="math" src="../../../../images/math/b353a8e69eea8509aa5b4a2e76cae1dfc128a5bf.png" alt="P = \frac{1}{k}"/>.</p>
<p>For example, <img class="math" src="../../../../images/math/fc72cad06de27ea3d4c668bffae632a535edcc52.png" alt="P( wind = string | tennis = no)"/>, we note wind has two possible values, so the uniform priors means <img class="math" src="../../../../images/math/9e4711ca7ee56c4e68c21e3adaa0576e5e93cc35.png" alt="P = \frac{1}{2}"/>.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">05 - Bayesian Learning</a><ul>
<li><a class="reference internal" href="#discrete-random-variables">Discrete Random Variables</a></li>
<li><a class="reference internal" href="#probabilities">Probabilities</a></li>
<li><a class="reference internal" href="#axioms-of-probability-theory">Axioms of Probability Theory</a></li>
<li><a class="reference internal" href="#theorems-from-the-axioms">Theorems from the Axioms</a></li>
<li><a class="reference internal" href="#multivalued-random-variables">Multivalued Random Variables</a></li>
<li><a class="reference internal" href="#conditional-probability">Conditional Probability</a></li>
<li><a class="reference internal" href="#definition-of-conditional-probability">Definition of Conditional Probability</a></li>
<li><a class="reference internal" href="#probabilistic-inference">Probabilistic Inference</a></li>
<li><a class="reference internal" href="#bayes-rule">Bayes Rule</a></li>
<li><a class="reference internal" href="#id1">Bayesian Learning</a></li>
<li><a class="reference internal" href="#an-illustrating-example">An Illustrating Example</a></li>
<li><a class="reference internal" href="#choosing-hypotheses">Choosing Hypotheses</a></li>
<li><a class="reference internal" href="#maximum-a-posteriori-map">Maximum a Posteriori (MAP)</a></li>
<li><a class="reference internal" href="#maximum-likelihood-ml">Maximum Likelihood (ML)</a></li>
<li><a class="reference internal" href="#does-the-patient-have-cancer-or-not">Does the Patient Have Cancer or Not?</a></li>
<li><a class="reference internal" href="#bayesian-classifier">Bayesian Classifier</a></li>
<li><a class="reference internal" href="#naive-bayes-classifier">Naïve Bayes Classifier</a></li>
<li><a class="reference internal" href="#back-to-the-example">Back to the Example</a></li>
<li><a class="reference internal" href="#estimating-probabilities">Estimating Probabilities</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="04.html"
                        title="previous chapter">04 - Multilayer Perceptrons</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="06.html"
                        title="next chapter">06 - K-Nearest Neighbour Classifier</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../../../sources/modules/part2/semester1/G53MLE/05.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="06.html" title="06 - K-Nearest Neighbour Classifier"
             >next</a> |</li>
        <li class="right" >
          <a href="04.html" title="04 - Multilayer Perceptrons"
             >previous</a> |</li>
        <li><a href="../../../../index.html">Lecture Notes v1.0 documentation</a> &raquo;</li>
          <li><a href="index.html" >G53MLE Machine Learning</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010, Marcus Whybrow.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.4.
    </div>
  </body>
</html>