

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>02 - Perceptron &mdash; Lecture Notes v1.0 documentation</title>
    <link rel="stylesheet" href="../../../../static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../../../static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../static/jquery.js"></script>
    <script type="text/javascript" src="../../../../static/underscore.js"></script>
    <script type="text/javascript" src="../../../../static/doctools.js"></script>
    <link rel="top" title="Lecture Notes v1.0 documentation" href="../../../../index.html" />
    <link rel="up" title="G53MLE Machine Learning" href="index.html" />
    <link rel="next" title="03 - ADALINE and Delta Rule" href="03.html" />
    <link rel="prev" title="01 - Introduction" href="01.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="03.html" title="03 - ADALINE and Delta Rule"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="01.html" title="01 - Introduction"
             accesskey="P">previous</a> |</li>
        <li><a href="../../../../index.html">Lecture Notes v1.0 documentation</a> &raquo;</li>
          <li><a href="index.html" accesskey="U">G53MLE Machine Learning</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="perceptron">
<span id="g53mle02"></span><h1>02 - Perceptron<a class="headerlink" href="#perceptron" title="Permalink to this headline">¶</a></h1>
<p>Over the next few lectures we will study different methods of machine learning.</p>
<div class="section" id="perceptron-basic">
<h2>Perceptron Basic<a class="headerlink" href="#perceptron-basic" title="Permalink to this headline">¶</a></h2>
<p>A Perceptron is a type of artificial neural network or ANN. It computes the sum of its weighted inputs and passes the result to a hard-limit threshold function.</p>
<p>It was designed to represent a single neurone in the brain. No one can be sure what exactly a neurone does but this method has some nice properties anyway.</p>
</div>
<div class="section" id="perceptron-operation">
<h2>Perceptron Operation<a class="headerlink" href="#perceptron-operation" title="Permalink to this headline">¶</a></h2>
<p>A perceptron ACC accepts an input vector (list of numbers) of <tt class="docutils literal"><span class="pre">n</span></tt> size and their weights and also a bias number and calculates a linear combination of these inputs and outputs either a 1 or a -1.</p>
<p>First a <strong>weighted sum</strong> is created which is the sum of all the inputs (<img class="math" src="../../../../images/math/67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png" alt="x_i"/>) multiplied by their weights (<img class="math" src="../../../../images/math/535b2bfbb0a587e261a0d0af9b7b53e42629b14d.png" alt="w_i"/>) plus an additional bias weight (<img class="math" src="../../../../images/math/eb37b8f84b267a3318413dcd963c9a5872051634.png" alt="w_0"/>):</p>
<div class="math">
<p><img src="../../../../images/math/dab1c67a7a0941ff969e0d5e65ecea6000ed1567.png" alt="w_{0} + \sum_{i=1}^{n} w_{i}x_{i}" /></p>
</div><p>Then the weighted sum is passed through a threshold and ether is outputted as 1 or -1.</p>
</div>
<div class="section" id="perceptron-decision-surface">
<h2>Perceptron Decision Surface<a class="headerlink" href="#perceptron-decision-surface" title="Permalink to this headline">¶</a></h2>
<p>The decision surface is one which is understandable in the 1D, 2D and 3D space. It would be a point, line and plane respectively. Decision surfaces can still be  represented in <strong>hyper feature space</strong> (more than 3 dimensions) and are called <strong>hyperplane decision surfaces</strong>.</p>
<p>The perceptron outputs a 1 (or fires an electron in the brains case) for instances which lie on one side of the hyperplane and a -1 (the neurone doesn&#8217;t fire in the brain) for instances lying on the other side.</p>
<p>For instances with 2 features we can represent the feature space in euclidian space (a graph commonly labeled with an x and y axis containing points or lines).</p>
<p>Ordinarily the hypothesis consists of a bunch of inputs (<img class="math" src="../../../../images/math/67bc6daa9d6b964201d6cef60cbeb1ac5fd26ead.png" alt="x_i"/>) and a bunch of matching weights (<img class="math" src="../../../../images/math/535b2bfbb0a587e261a0d0af9b7b53e42629b14d.png" alt="w_i"/>) including a weight <img class="math" src="../../../../images/math/eb37b8f84b267a3318413dcd963c9a5872051634.png" alt="w_0"/> known as a bias which never changes as the other weights do. The weights and inputs are summed up in the following way for a 2 dimensional feature vector:</p>
<div class="math">
<p><img src="../../../../images/math/45464210372e311bc04d8eac256bc1af4b5b8c84.png" alt="w_0 + w_1 x_1 + w_2 x_2" /></p>
</div><p>When we want to draw the decision surface (in this case it is not a hyperplane but a simple visualisable line) we say that the result should be equal to 0:</p>
<div class="math">
<p><img src="../../../../images/math/292be5b3b933ee5ba2dd458d91d84d1fd4a4806d.png" alt="w_0 + w_1 x_1 + w_2 x_2 = 0" /></p>
</div><p>In order to represent this more closely to a euclidean line we might rearrange it thus:</p>
<div class="math">
<p><img src="../../../../images/math/cfb6a825179c71b4e1f9e2701ee012415cf3bdae.png" alt="w_2 x_2 = - w_1 x_1 - w_0" /></p>
</div><p>Which is similar to the format <img class="math" src="../../../../images/math/2577acc965dc2242d91cbd2f4245aed73e425461.png" alt="y = ?x + ?"/> commonly seen in euclidian space.</p>
<p>If for example we were trying to guess the weights for a hypothesis which created a decision surface which looked like this:</p>
<img alt="../../../../images/euclidean-feature-space.png" src="../../../../images/euclidean-feature-space.png" />
<p>Then assuming the line intercepts the vertical axis at lets say 1, and that the gradient is -1 (for every unit traveled in the positive direction right we travel one unit in the negative direction down thus <img class="math" src="../../../../images/math/6ad1bfa29a2464ec1a1122c28669bd3f7ff62172.png" alt="\frac{1}{-1} = -1"/>) the equation would look like so:</p>
<div class="math">
<p><img src="../../../../images/math/c5212768b457c4122d0676abc24e9eeed3f72928.png" alt="x_2 = - x_1 + 1" /></p>
</div><p>Thus by rearranging the equation in order make it equal to 0 agin we can determine the values of weights of the hypothesis:</p>
<div class="math">
<p><img src="../../../../images/math/013d683ec838d86b8636e8a626723836b351edd7.png" alt="x_2 + x_1 - 1 = 0" /></p>
</div><p>So we know that <img class="math" src="../../../../images/math/0b2d26a426d1074257ae3703c9679d0316e52260.png" alt="w_2 = 1"/>, <img class="math" src="../../../../images/math/c5c00ce3d5af013bd6c23a855add41bb0d494ec4.png" alt="w_1 = 1"/> and <img class="math" src="../../../../images/math/43432500d45398242ee12bd3c7af9a1bd29403e1.png" alt="w_0 = - 1"/>, since <img class="math" src="../../../../images/math/0ce0501dff8a93818a4e5d9ecc570d8ab6472e5b.png" alt="w_2"/> is the coefficient of <img class="math" src="../../../../images/math/6a7d010bbff66a0c41e43310a51efbaa6bf63396.png" alt="x_2"/>, <img class="math" src="../../../../images/math/3c352e9db5a7134ca93f9fecce8fe99b0d4e5889.png" alt="w_1"/> is the coefficient of <img class="math" src="../../../../images/math/ccada11db7b2b90693e2fac4f887a57fce6f96bf.png" alt="x_1"/> and <img class="math" src="../../../../images/math/eb37b8f84b267a3318413dcd963c9a5872051634.png" alt="w_0"/> is the constant.</p>
<div class="section" id="so-what-can-we-represent">
<h3>So What Can we Represent?<a class="headerlink" href="#so-what-can-we-represent" title="Permalink to this headline">¶</a></h3>
<p>We can represent many boolean functions if we assume a True result to return a 1 and a False result to return a -1. For example the boolean operator AND would look like so:</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"><img class="math" src="../../../../images/math/ccada11db7b2b90693e2fac4f887a57fce6f96bf.png" alt="x_1"/></th>
<th class="head"><img class="math" src="../../../../images/math/6a7d010bbff66a0c41e43310a51efbaa6bf63396.png" alt="x_2"/></th>
<th class="head"><img class="math" src="../../../../images/math/96ab646de7704969b91c76a214126b45f2b07b25.png" alt="d"/></th>
</tr>
</thead>
<tbody valign="top">
<tr><td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr><td>-1</td>
<td>+1</td>
<td>-1</td>
</tr>
<tr><td>+1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr><td>+1</td>
<td>+1</td>
<td>+1</td>
</tr>
</tbody>
</table>
<img alt="../../../../images/and-feature-space.png" src="../../../../images/and-feature-space.png" />
<p>Essentially a perceptron can solve any problem where the objects are linearly separable:</p>
<img alt="../../../../images/linearly-separable.png" src="../../../../images/linearly-separable.png" />
<p>However some problems are <strong>linearly non-sparable</strong>. A good example of this is the exclusive or function or XOR:</p>
<table border="1" class="docutils">
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head"><img class="math" src="../../../../images/math/ccada11db7b2b90693e2fac4f887a57fce6f96bf.png" alt="x_1"/></th>
<th class="head"><img class="math" src="../../../../images/math/6a7d010bbff66a0c41e43310a51efbaa6bf63396.png" alt="x_2"/></th>
<th class="head"><img class="math" src="../../../../images/math/96ab646de7704969b91c76a214126b45f2b07b25.png" alt="d"/></th>
</tr>
</thead>
<tbody valign="top">
<tr><td>-1</td>
<td>-1</td>
<td>-1</td>
</tr>
<tr><td>-1</td>
<td>+1</td>
<td>+1</td>
</tr>
<tr><td>+1</td>
<td>-1</td>
<td>+1</td>
</tr>
<tr><td>+1</td>
<td>+1</td>
<td>-1</td>
</tr>
</tbody>
</table>
<p>XOR produces a graph which looks like this where it is impossible to separate the +1&#8217;s from the -1&#8217;s by a straight line:</p>
<img alt="../../../../images/linearly-non-separable.png" src="../../../../images/linearly-non-separable.png" />
</div>
</div>
<div class="section" id="training-algorithms">
<h2>Training Algorithms<a class="headerlink" href="#training-algorithms" title="Permalink to this headline">¶</a></h2>
<p>In problems where the answer <em>is</em> linearly separable, a perceptron must learn where the line (decision surface) is best placed to separate the feature space.</p>
<p>This is achieved by presenting the perceptron with training pairs known as an experience (X,D) where X is the set of vector (meaning list of numbers) training instances (input) and D is the set of vector classifications (+1 or -1) for each input. These experiences are presented to the perceptron for training <em>one at a time</em> until the process converges.</p>
<p>The procedure is as follows:</p>
<ol class="arabic">
<li><p class="first">Set the weights to small random values (e.g. in the rang -1 to 1)</p>
</li>
<li><p class="first">Present X to the perceptron and calculate the <img class="math" src="../../../../images/math/3df21786c9e165df0ded522bb36d830b7d9a453c.png" alt="output"/> (+1 or -1) by the following process:</p>
<blockquote>
<p><img class="math" src="../../../../images/math/cc1124120435c843174c1bd246af8fb7054e603e.png" alt="output = threshold(R)"/></p>
<p>where:</p>
</blockquote>
</li>
</ol>
<div class="math">
<p><img src="../../../../images/math/ddfbae8a3f71339be55abdab87f160566756d834.png" alt="R = w_0 + \sum_{i=1}^n e_i x_i" /></p>
</div><div class="math">
<p><img src="../../../../images/math/142392a51e9eb67d7d55b186cb5e7f6852942166.png" alt="threshold =
\left\{
    \begin{array}{ll}
        +1 &amp; \mbox{if } R &gt; 0 \\
        0  &amp; \mbox{otherwise }
    \end{array}
\right." /></p>
</div><ol class="arabic">
<li><p class="first">Update the weights for <img class="math" src="../../../../images/math/40062b8e7106a0b59f1bc2533bec962b4fb784cf.png" alt="i = 1,2,...,n"/>:</p>
<blockquote>
<p><img class="math" src="../../../../images/math/3374407d48df36ef2c253e08935177a216a15c65.png" alt="w_i \leftarrow w_i + \eta (d - y) x_i"/></p>
<p>where <img class="math" src="../../../../images/math/f0949fde45227d05dcb1f33148e8385bd87ed42f.png" alt="0 &lt; \eta &lt; 1"/> is a training rate</p>
</blockquote>
</li>
<li><p class="first">Repeat by going to step 2.</p>
</li>
</ol>
<p>Once the weight changing algorithm has been through each instance of the training data this is known as a single <strong>Epoch</strong>.</p>
<div class="section" id="convergence-theorem">
<h3>Convergence Theorem<a class="headerlink" href="#convergence-theorem" title="Permalink to this headline">¶</a></h3>
<p>There perceptron training rule discussed will eventually converge on a single hypothesis from the hypothesis space of lines finding a <strong>weight vector</strong> which correctly classifies all training samples within a finite number of iterations, <strong>provided the training examples are linearly separable</strong> and provided a sufficiently small <img class="math" src="../../../../images/math/03a45952f8115322d2879f7d090126f059757ba0.png" alt="\eta"/> is used.</p>
<p>Unfortunately theory and real life can differ very much and things may not be as simple as they can be described mathematically.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">02 - Perceptron</a><ul>
<li><a class="reference internal" href="#perceptron-basic">Perceptron Basic</a></li>
<li><a class="reference internal" href="#perceptron-operation">Perceptron Operation</a></li>
<li><a class="reference internal" href="#perceptron-decision-surface">Perceptron Decision Surface</a><ul>
<li><a class="reference internal" href="#so-what-can-we-represent">So What Can we Represent?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training-algorithms">Training Algorithms</a><ul>
<li><a class="reference internal" href="#convergence-theorem">Convergence Theorem</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="01.html"
                        title="previous chapter">01 - Introduction</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="03.html"
                        title="next chapter">03 - ADALINE and Delta Rule</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="../../../../sources/modules/part2/semester1/G53MLE/02.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" size="18" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="03.html" title="03 - ADALINE and Delta Rule"
             >next</a> |</li>
        <li class="right" >
          <a href="01.html" title="01 - Introduction"
             >previous</a> |</li>
        <li><a href="../../../../index.html">Lecture Notes v1.0 documentation</a> &raquo;</li>
          <li><a href="index.html" >G53MLE Machine Learning</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2010, Marcus Whybrow.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.4.
    </div>
  </body>
</html>