.. _G53MLE05:

======================
05 - Bayesian Learning
======================

Discrete Random Variables
=========================

If :math:`A` is a boolean valued random variable denoting an event, and there is some degree of uncertainty as to whether :math:`A` occurs.

For example :math:`A` could be:

* The US president in 2023 will be Male
* You wake up tomorrow with a headache
* You have Ebola

Probabilities
=============

We write :math:`P(A)` as "the fraction of possible worlds in which :math:`A` is true".

Axioms of Probability Theory
============================

#. All probabilities lie between 0 and 1

    .. math:: 0 <= P(A) <= 1

#. A true proposition has probability 1, and a false proposition has probability 0

    .. math::
        P(true) = 1
        
        P(false) = 0

# THe probability of disjunction is:

    .. math:: P(A \vee B) = P(A) + P(B) - P(A \wedge B)

Theorems from the Axioms
========================

.. math::

    P( \lnot A) = P( \sim A) = 1 - P(A)
    
    P(A) = P(A \wedge B) + P(A \wedge \sim B)

Multivalued Random Variables
============================

Suppose :math:`A` can take on exactly one value of the set :math:`{v_1, v_2, ..., v_k}`:

.. math::

    P(A = v_i \wedge A = v_j) = 0 if i \neq j
    
    P(A = v_1 \vee A = V-2 \vee ... \vee A = v_k) = 1 = \sum_{j=1}^i P(A = v_j)

Conditional Probability
=======================

The probability :math:`P(A|B)` is the fraction of worlds in which :math:`B` is true that also have A as true.

For example:
* :math:`H` is "Having a headache"
* :math:`F` is "Coming down with Flu"

If:

.. math::
    P(H) = \frac{1}{10}
    P(F) = \frac{1}{40}
    P(H|F) = \frac{1}{2}

Remember that :math:`P(H|F)` means "what is the probability of :math:`H` if :math:`F` is true".

Definition of Conditional Probability
=====================================

.. math:: P(H|F) = \frac{P(H \wedge F)}{P(F)}

And also *The Chain Rule*

.. math:: P(A \wedge B) = P(A|B) P(B)

Probabilistic Inference
=======================

It is **not** good reasoning to say that "I have woken up with a headache, thus there is a 50% chance I have of coming down with the flu since 50% of flus lead to a headache".

.. math:: P(F \wedge H) = P(H|F) P(F) = \frac{1}{80}

However:

.. math:: P(F|H) = \frac{P(F \wedge H)}{P(H)} = \frac{\frac{1}{80}}{\frac{1}{10}} = \frac{1}{8}

Bayes Rule
==========

**Bayes, Thomas (1763)** AN essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London.

So Bayes Rule is:

.. math:: P(A|B) = \frac{P(A \wedge B)}{P(B)} = \frac{P(V|A)P(A)}{P(B)}

Bayesian Learning
=================

So we can use this rule in a machine learning context by first renaming the variable more appropriately:

.. math:: P(h|x) = \frac{P(x|h)P(h)}{P(x)}

where :math:`x` is the data and :math:`h` is the hypothesis.

* :math:`P(h)` is the probability of hypothesis :math:`h` before seeing any data.
* :math:`P(x|h)` is the probability of the data if the hypothesis :math:`h` is true.
* :math:`P(h|x)` is the probability of hypothesis :math:`h` after having see the data :math:`x`.

*...*