.. _G53MLE03:

===========================
03 - ADALINE and Delta Rule
===========================

ADAptive LINear Element (ADALINE) vs Perceptron.

ADALINE just outputs the weighted sum directly as apposed to the Perceptron with passes that weighted sum through a threshold as well.

The ADALINE and Delta Rule
==========================

When the problem is not linearly separable the perceptron will fail to converge. ADALINE can overcome this difficulty by finding a best fit approximation to the target.



The ADALINE Error Function
==========================

We have training paris :math:`(X_k, d_k)` for :math:`k=1,2,...,K` where K is the number of training samples, the training error specifies the difference between the output of the ADLINE and the desired target.

The error function :math:`E` is defined as:

.. math:: E (W) = \frac{1}{2} \sum_{k=1}^K (d_k - y_k)^2

where :math:`d_k` is the desired output and :math:`y_k` is the actual output from the ADLINE neurone, both for experience :math:`k`.

**Clarification:**

The ADALINE neurone still calculates its output in the same way as the perceptron did, however without passing it through a threshold function:

.. math:: Y = w_0 + \sum_{i=1}^n w_i x_i

where :math:`n` is the number of inputs. This can be represented more simply. If we instead take :math:`w_0` and append it onto the end of the weight vector :math:`w` and append a 1 to the end of the inputs vector :math:`x` we can simply write:

.. math:: Y = \sum_{i=1}^n w_i x_i

since for :math:`i=n`, :math:`w_n x_n` will be :math:`w_0 \times 1`, which will be summed together with the other values. Mathematically this is a common format which is given the name **dot product**, which is denoted :math:`w \cdot x`, so these are all the same thing:

.. math:: w \cdot x = w_1 x_1 + w_2 x_2 + ... + w_n x_n = \sum_{i=1}^n w_i x_i

So lets take an example, if we have a 3 dimensional input vector say :math:`\left[10,20,30\right]` and a bias (formally :math:`w_0`) of 5 and current weights for the 3 dimensional input vector :math:`\left[0.1,0.2,0.3\right]` we would (using the :math:`\Sigma` based sum function) have the following:

.. math:: y = 5 + (0.1 \times 10) + (0.2 \times 20) + (0.3 \times 30)

If we convert the input and weight vectors to incorporate the bias we can use the dot product instead. So the inputs vector would be :math:`\left[10,20,30,1\right]` and the weights vector would be :math:`\left[0.1,0.2,0.3,5\right]`, now we get:

.. math:: y = \left[0.1,0.2,0.3,5\right] \cdot \left[10,20,30,1\right] = (0.1 \times 10) + (0.2 \times 20) + (0.3 \times 30) + (5 \times 1)

**OK Back to the Error function stuff:**

The difference between the actual output and the designed output (:math:`(d_k - y_k)^2`) is used by the error function in order to determine how close we got to approximating the desired function/weights.

The smaller :math:`E(W)` is the closer the approximation is to the perfect function.

So the training task is to find :math:`W` such that :math:`E(W)` is minimised.

The differing values of :math:`W` creates a so called *error surface* which is visualisable if there are 2 or 3 weights. In 2D this surface would appear as valleys and mountains etc. spanning the hypothesis space. Our aim is to find the point which is the lowest in that error surface.

The Gradient Descent Rule
=========================

If you imagine a 2D graph displaying the error surface (in this case some sort of curved line) we need to move :math:`w(0)` to a new value, such that :math:`E(w(new)) < E(w(0))`.

This is easy if you are looking at the line, you can visually pick out the lowest point. To do this programmatically and without a priori knowledge we want to travel down negative gradients.

This equates to moving in the opposite direction to the gradient, i.e. if the gradient going forward is positive (going up) we must move in the negative direction (backwards), which will take us downwards.

Mathematically we can say:

.. math:: w_{new} = w_{old} - \Delta sign 

.. math:: \bigtriangledown E(W) = \left[\frac{\delta E}{ \delta w_1}, \frac{\delta E}{ \delta w_2}, ... , \frac{\delta E}{ \delta w_n}\right]

The gradient training rule is for each individual weight is:

.. math:: w_i = w_i - \eta \frac{\delta E}{\delta w_i}

where :math:`\eta` is the training rate.

The gradient descent training procedure is:

#. Initialise :math:`w_i` to small values (for example in the range of -1 to 1) and choose a learning rate (e.g. :math:`\eta = 0.1`).
#. Until the termination condition is met, do
    * For all training sample pairs :math:`<X_k, d_k>` input the instance :math:`X_k` and compute:
    
    .. math:: \delta_i = i \sum_{k=1}^K (d_k - y_k) x_i(k)
    
    * For each weight :math:`w_i`, do
    
        :math:`w_i = w_i + n \delta_i`

Stochastic (Incremental) Gradient Descent
=========================================

Also known as online mode, Least Mean Square (LMS), Widrow-Hoff, and the Delta Rule the process is as follows:

#. Initialise all :math:`w_i` to small values (e.g. in the range -1 to +1), choose a learning rate (e.g. :math:`\eta = 0.01`, which should be a smaller value than the one used in batch mode).

#. Until the termination condition is met, do:

    #. For **each** training sample pair :math:`(X_k, d_k)` compute:
    
        :math:`\delta_i = - (d_k - y_k) X_{k_i}`
    
    #. Then for each weight :math:`w_i`, do:
    
        :math:`w_i \leftarrow w_i + \eta \delta_i`

Training Iterations, Epochs
===========================

Training is an iterative process, thus training samples are used repeatedly for training. Assuming we have :math:`K` training samples :math:`(X_k, d_k)` for :math:`k=1,2,...,K`, then an epoch is the presentation of all :math:`K` training samples to the function approximation process **once**.

The order of training sample presentation between epochs can (and should) be different, and training may take many epochs to complete.

Termination of Training
=======================

To terminate training, there are normally two ways:

#. When a pre-set number of training epochs is reached
#. When the error is smaller than a pre-set value

Further Reading
===============

Chapter 4, Machine Learning, T. M. Mitchell 1997

