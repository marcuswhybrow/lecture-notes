.. _G53MLE07:

====================
07 - Data Clustering
====================

Neural network, K-nearest-neighbour and Bayesian learning algorithms are all **supervised** learning algorithms. This means that the correct output is made known to the machine by what we might call a teacher.

With **unsupervised** learning we cannot know whether an answer given is correct or incorrect. Instead we can achieve different goals such as, patterns, relations, and knowledge. This is called **data mining**.

Motivating Problems
===================

With a *true colour* image 24 bits are used per pixel (8 bits per channel) providing 1677216 possible colours. A *gif* image uses 8 bits per pixel giving a total of 256 possible colours.

Gifs work by having a table of 256 entries each specifying a red, green and blue value. A reference for each pixel in the image can then be stored (to one of the colour entries in the table) instead of the actual RGB value.

So the problem is how do we choose the pixels to be in the table...

If we take the red, green and blue values of each pixel in the image and use those values as the dimensions in euclidean space we will begin to see groupings of similar colours in 3D space.

Then we can replace gatherings of similar colours (points in euclidean space which are close to each other) with a single colour.

Once the number of euclidean points have been reduced to 256 we can create the gif image.

K-Means
=======

An algorithm for partitioning (or clustering) :math:`N` data points into :math:`K` disjoint subsets :math:`S_j` containing :math:`N_j` data points.

Suppose that our data has :math:`n` dimensions :math:`[x_{i1}, x_{i2}, ..., x_{in}]` for :math:`i = 1, 2, ..., N` for :math:`N` data points.

We want to cluster these :math:`N` points into :math:`K` subsets (clusters), where :math:`K` is preset (:math:`K` would be 256 in the gif example).

For each cluster, we define a prototype point :math:`M_j = [m_{j1}, m_{j2}, ..., m_{jn}]` where :math:`j = 1, 2, ..., K`, which can be set randomly the first time.

The distance between all euclidean points :math:`x_i` and each cluster prototype :math:`M_j` is calculates as:

.. math:: D(x_i, M_j) = | x_i - M_j |^2 = \sum_{l=1}^n (x_{il} - m_jl)^2

A euclidian point :math:`x_i` is assigned to the jth cluster :math:`C_j` (meaning :math:`x_i \epsilon C_j`) if the following condition holds:

*...*

J-Means Algorithm
=================

Step 1
******


Arbitrarily choose from the given sample set :math:`k`, some initial cluster centres.

.. math:: m_{0j} = [m^0_{j1}, m^0_{j2}, ..., m^0_{jn} j = 1, 2, ..., k]

There are no known best ways to decide the sample set, but :math:`k` could be the first :math:`k` samples of the sample set or be generated randomly.

We then say that :math:`t = 0` where :math:`t` is the *iteration index*.

Step 2
******

Assign each of the samples (:math:`x_i = [x_{i1}, x_{i2}, ..., x_{in}] i = 1,2,...,N`) to one of the clusters according to the distance between each sample and the centre of each cluster.

.. math::
    x_i \epsilon C^t_j
    
    if D(x_i, M^t_j) \leq D(x_i, M^t_l)
    
    for all l = 1,2,...,k

Step 3
******

Update the cluster centres to get:

.. math::
    M^{t+1}_j = [m^{t+1}_{j1}, m^{t+1}_{j2}, ..., m^{t+1}_{jn}] j = 1,2,...,K

according to

.. math:: M^{t+1}_j = \frac{1}{N^t_j} \sum_{x_i \epsilon C^t_j} x_i

where :math:`N^t_j` is the number of samples in :math:`C^t_j`

Step 4
******

Calculate the error of approximation:

.. math:: E_t = \frac{1}{2} \sum^K_{j=1} \sum_{x_i \epsilon C^t_j} | x_i - M^t_j |^2

Step 5
******

If the terminating criterion is met, then stop. Otherwise :math:`t = t+1` and go back to *step 2*.

**Stopping Criterions**

The K-means algorithm can be stopped based upon the following criterions:

#. The errors do not change significantly in two consecutive epochs

    For example :math:`|E_t - E_{t-1} | \le \sigma` where :math:`\sigma` is some preset small value.

#. No further change in the assignment of the data points to clusters in two consecutive epochs

#. Stop after fixed number of epochs regardless of the error.

What Does K-Means Do?
=====================

It tries to find the centre vectors :math:`M_j`'s that optimise the following cost function:

.. math:: E_t = \frac{1}{2} \sum^K_{j=1} \sum_{x_i \epsilon C^t_j} | x_i - M^t_j |^2

Some Remarks
============

* K-Means is a gradient descent algorithm, trying to minimise a cost function :math:`E`.
* In general, the algorithm does not achieve a global minimum of :math:`E` over the assignments.
* It is sensitive to the initial choice of cluster centres. Different starting cluster centroids may lead to different solutions.
* It is a popular method, many more advanced methods have been derived from this simple algorithm.

Competitive Learning Neural Networks
====================================

This type of network usually has one layer of fan-out units and one layer of processing units:

.. image:: /media/G53MLE/07/images/neuralnetwork.png

The processing layer consists of :math:`M` processing units, each receiving :math:`N` input signals from the fan-out units. The :math:`x_i` input for processing unit :math:`j` has weight :math:`w_{ij}` assigned to it.

The output of the processing units compete on the basis of which of them has its weight vector :math:`W_j = [w_{j1}, w_{j2}, ..., w_{jN}]` for all :math:`j`, closest to the input vector :math:`X` (as measured by a distance function D).

The winning unit generates an output signal of 1; all the other units having outputs of 0.

Each processing units calculates the distance between the input vector and the weight vector connecting the input to it, the activation of the ith processing unit is:

.. math:: a_i = D(W_i, X) i = 1,2,...,M

:math:`D(W_i, X)` is a distance measurement function. The most common choice for this is the euclidean distance.

Once each processing unit has calculated its activation, a competition takes place to see which output unit has the smallest activate value.

This implies finding the unit which has its associated weight vector closest to the input vector :math:`X`. THe unit with the smallest activation value is declared as the winner, all other units are losers.

* The aim of such a network is to cluster the input data.
* Simpler inputs should be classified as being in the same cluster
* There is no know desired outputs
* The outputs are found by the network itself from the correlation of the input data
* Such a network is also called a **self-organising** or an **un-supervising** neural network.
* This is unsupervised competitive learning


Competitive Learning Algorithms
===============================

The task is to classify objects in the feature space into clusters.

*...*

Competitive Learning
********************

For a given training instance, the distance to all clusters is calculated. The closest cluster vector is chosen, and its weights **only** are updated.

*...*
